# -*- coding: utf-8 -*-
"""Activations.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mbcv6BrQRq_esGgsFIYFN7Ia24KdzjN7
"""

import numpy as np

################################################
#         Activations
################################################

class Sigmoid():
    """
    Sigmoid activation function.

    Args:
        c (float): Scaling factor (default 1).
        b (float): Bias term (default 0).
    """
    def __init__(self, c=1, b=0):
        self.c = c
        self.b = b

    def value(self, x):
        """
        Compute the sigmoid function.

        Args:
            x (numpy.ndarray): Input values.

        Returns:
            numpy.ndarray: Sigmoid values.
        """
        val = 1 + np.exp(-self.c * (x + self.b))
        return 1 / val

    def diff(self, x, remove=False):
        """
        Compute the derivative of the sigmoid function.

        Args:
            x (numpy.ndarray): Input values.
            remove (bool): Whether to remove the last dimension (default False).

        Returns:
            numpy.ndarray: Derivative of the sigmoid function.
        """
        y = self.value(x)
        if remove == True:
            y = y[:-1, :]
        val = self.c * y * (1 - y)
        return val

class Tanh():
    """
    Hyperbolic tangent (tanh) activation function.
    """
    def __init__(self):
        pass

    def value(self, x):
        """
        Compute the tanh function.

        Args:
            x (numpy.ndarray): Input values.

        Returns:
            numpy.ndarray: Tanh values.
        """
        num = np.exp(x) - np.exp(-x)
        denom = np.exp(x) + np.exp(-x)
        return num / denom

    def diff(self, x):
        """
        Compute the derivative of the tanh function.

        Args:
            x (numpy.ndarray): Input values.

        Returns:
            numpy.ndarray: Derivative of the tanh function.
        """
        y = self.value(x)
        val = 1 - y**2
        return val

class Relu():
    """
    Rectified Linear Unit (ReLU) activation function.
    """
    def __init__(self):
        pass

    def value(self, x):
        """
        Compute the ReLU function.

        Args:
            x (numpy.ndarray): Input values.

        Returns:
            numpy.ndarray: ReLU values.
        """
        val = x
        val[val < 0] = 0
        return val

    def diff(self, x):
        """
        Compute the derivative of the ReLU function.

        Args:
            x (numpy.ndarray): Input values.

        Returns:
            numpy.ndarray: Derivative of the ReLU function.
        """
        val = np.ones(x.shape)
        val[val <= 0] = 0
        return val

class Softmax():
    """
    Softmax activation function.
    """
    def __init__(self):
        pass

    def value(self, x):
        """
        Compute the softmax function.

        Args:
            x (numpy.ndarray): Input values.

        Returns:
            numpy.ndarray: Softmax values.
        """
        val = np.exp(x) / np.sum(np.exp(x), axis=0)
        return val

    def diff(self, x):
        """
        Compute the derivative of the softmax function.

        Args:
            x (numpy.ndarray): Input values.

        Returns:
            numpy.ndarray: Derivative of the softmax function.
        """
        y = self.value(x)
        mat = np.tile(y, y.shape[0])
        val = np.diag(y.reshape(-1, )) - (mat * mat.T)
        return val