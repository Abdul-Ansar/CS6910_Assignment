# -*- coding: utf-8 -*-
"""Question_07 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pdFN2HyOLF-QoLYiifX_lmJztn9JKDPI
"""

# Importing necessary libraries
print("Importing Libraries ...")
import wandb  # For experiment tracking
import numpy as np  # For numerical operations
import os  # For operating system related functionalities
from activations import Sigmoid, Tanh, Relu, Softmax  # Custom activation functions
from layers import Input, Dense  # Custom layer implementations
from optimizers import Normal, Momentum, Nesterov, AdaGrad, RMSProp, Adam, Nadam  # Custom optimizer implementations
from network import NeuralNetwork  # Custom neural network implementation
from loss import CrossEntropy, SquaredError  # Custom loss functions
from helper import OneHotEncoder, MinMaxScaler  # Helper functions for data preprocessing
from sklearn.metrics import confusion_matrix  # For confusion matrix
from sklearn.model_selection import train_test_split  # For splitting dataset
from keras.datasets import fashion_mnist  # Importing Fashion MNIST dataset
import matplotlib.pyplot as plt  # For plotting
import seaborn as sns  # For visualization
print("Done!")  # Indicating successful library import

#################################################################### [markdown]
# # Loss on Training Data
####################################################################

# Loading Fashion MNIST dataset
print("Loading data ... ", end="")
[(x_train, y_train), (x_test, y_test)] = fashion_mnist.load_data()  # Loading data
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)  # Splitting data
print("Done!")  # Indicating successful data loading

# Printing sizes of training and validation datasets
print("Size of Training data:", x_train.shape)
print("Size of Validation data:", x_val.shape)

# Performing scaling and encoding transformations on the data
print("Performing Scaling and Encoding transformations on the data ... ", end="")
scaler = MinMaxScaler()
scaler.fit(x_train)  # Fitting scaler on training data
X_scaled = x_train / 255  # Scaling training data
X_val_scaled = x_val / 255  # Scaling validation data
X_test_scaled = x_test / 255  # Scaling test data

# Reshaping data for compatibility with neural network
X_scaled = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1] * X_scaled.shape[2]).T
X_val_scaled = X_val_scaled.reshape(X_val_scaled.shape[0], X_val_scaled.shape[1] * X_val_scaled.shape[2]).T
X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1] * X_test_scaled.shape[2]).T

# Encoding labels using OneHotEncoder
encoder = OneHotEncoder()
t = encoder.fit_transform(y_train, 10)  # Encoding training labels
t_val = encoder.fit_transform(y_val, 10)  # Encoding validation labels
t_test = encoder.fit_transform(y_test, 10)  # Encoding test labels
print("Done!")  # Indicating successful transformations

# Limiting data size for faster testing
X_scaled = X_scaled[:, :21000]
X_test_scaled = X_test_scaled[:, :9000]
t = t[:, :21000]
t_test = t_test[:, :9000]

# Configuration for hyperparameter sweep
sweep_config = {"name": "best-sweep", "method": "grid"}
sweep_config["metric"] = {"name": "loss", "goal": "minimize"}
parameters_dict = {
                "num_epochs": {"values": [1]}, \
                "size_hidden_layer": {"values": [64]}, \
                "optimizer": {"values": ["RMSProp"]}, \
                "batch_size": {"values": [128]}, \
                "weight_init": {"values": ["XavierUniform"]} , \
                "activation": {"values": ["Sigmoid"]}, \
                "loss": {"values": ["SquaredError"]}, \
                  }
sweep_config["parameters"] = parameters_dict

# Function to train neural network with different configurations
def train_nn(config=sweep_config):
    with wandb.init(config=config):
        config = wandb.init().config  # Getting configuration
        wandb.run.name = "e_{}_hl_{}_opt_{}_bs_{}_init_{}_ac_{}_loss_{}".format(config.num_epochs,\
                                                                      config.size_hidden_layer,\
                                                                      config.optimizer,\
                                                                      config.batch_size,\
                                                                      config.weight_init,\
                                                                      config.activation,\
                                                                      config.loss)

        layers = [Input(data=X_scaled),\
                  Dense(size=config.size_hidden_layer, activation=config.activation, name="HL1"),\
                  Dense(size=10, activation=config.activation, name="OL")]

        nn_model = NeuralNetwork(layers=layers, batch_size=config.batch_size, \
                                 optimizer=config.optimizer, initialization=config.weight_init, \
                                 epochs=config.num_epochs, t=t, X_val=X_val_scaled, \
                                 t_val=t_val, loss=config.loss, use_wandb=True)

        nn_model.forward_propogation()
        nn_model.backward_propogation()
        acc_val, loss_val, _ = nn_model.check_test(X_val_scaled, t_val)
        acc_test, loss_test, y_test_pred = nn_model.check_test(X_test_scaled, t_test)

        wandb.log({"val_loss_end": loss_val/t_val.shape[1], \
                   "val_acc_end": acc_val/t_val.shape[1], \
                   "test_loss_end": loss_test/t_test.shape[1], \
                   "test_acc_end": acc_test/t_test.shape[1], \
                   "epoch":config.num_epochs})

        # cf_matrix = confusion_matrix(y_test[:9000], y_test_pred)
        # # plt.figure()
        # sns.heatmap(cf_matrix)
        # plt.title("Confusion Matrix")
        # wandb.log({"Confusion Matrix": plt})

        data = [[x, y] for (x, y) in zip(y_test[:9000], y_test_pred)]
        table = wandb.Table(data=data, columns = ["true", "predicted"])

        wandb.log({"conf_mat" : wandb.plot.confusion_matrix(
                        probs=None,
                        y_true=y_test[:9000],
                        preds=y_test_pred,
                        class_names=["T-shirt/top","Trouser","Pullover","Dress","Coat","Sandal","Shirt","Sneaker","Bag","Ankle boot"])})

# Initiating the hyperparameter sweep
sweep_id = wandb.sweep(sweep_config, project="trail-1")
wandb.agent(sweep_id, function=train_nn)