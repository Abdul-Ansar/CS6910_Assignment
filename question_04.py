# -*- coding: utf-8 -*-
"""Question_04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12WW_7rhJPUqmyMZ_t5zIlrtBuoUOiXuF
"""

# Printing progress during library import
print("Importing Libraries ...")

# Importing necessary libraries
import wandb  # Importing Weights & Biases for experiment tracking
import numpy as np  # For numerical operations
import os  # For operating system related functionalities
from activations import Sigmoid, Tanh, Relu, Softmax  # Custom activation functions
from layers import Input, Dense  # Custom layer implementations
from optimizers import Normal, Momentum, Nesterov, AdaGrad, RMSProp, Adam, Nadam  # Custom optimizer implementations
from network import NeuralNetwork  # Custom neural network implementation
from loss import CrossEntropy, SquaredError  # Custom loss functions
from helper import OneHotEncoder, MinMaxScaler  # Helper functions for data preprocessing
from sklearn.model_selection import train_test_split  # For splitting dataset
from keras.datasets import fashion_mnist  # Importing Fashion MNIST dataset
import matplotlib.pyplot as plt  # For plotting

print("Done!")  # Indicating successful library import

# Loading Fashion MNIST dataset
print("Loading data ... ", end="")
[(x_train, y_train), (x_test, y_test)] = fashion_mnist.load_data()  # Loading data
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)  # Splitting data
print("Done!")  # Indicating successful data loading

# Printing sizes of training and validation datasets
print("Size of Training data:", x_train.shape)
print("Size of Validation data:", x_val.shape)

# Performing scaling and encoding transformations on the data
print("Performing Scaling and Encoding transformations on the data ... ", end="")
X_scaled = x_train / 255  # Scaling training data
X_val_scaled = x_val / 255  # Scaling validation data
X_test_scaled = x_test / 255  # Scaling test data

# Reshaping data for compatibility with neural network
X_scaled = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1] * X_scaled.shape[2]).T
X_val_scaled = X_val_scaled.reshape(X_val_scaled.shape[0], X_val_scaled.shape[1] * X_val_scaled.shape[2]).T
X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1] * X_test_scaled.shape[2]).T

# Encoding labels using OneHotEncoder
encoder = OneHotEncoder()
t = encoder.fit_transform(y_train, 10)  # Encoding training labels
t_val = encoder.fit_transform(y_val, 10)  # Encoding validation labels
t_test = encoder.fit_transform(y_test, 10)  # Encoding test labels
print("Done!")  # Indicating successful transformations

# Limiting data size for faster testing
X_scaled = X_scaled[:, :21000]
X_test_scaled = X_test_scaled[:, :9000]
t = t[:, :21000]
t_test = t_test[:, :9000]

####################################################################
# # Preparing small dataset to test the code
# [(X_train, y_train), (X_test, y_test)] = fashion_mnist.load_data()
# scaler = MinMaxScaler()
# X_scaled = scaler.fit_transform(X_train)
# X_scaled = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1]*X_scaled.shape[2]).T

# encoder = OneHotEncoder()
# t = encoder.fit_transform(y_train, 10)


####################################################################

# Configuration for hyperparameter sweep
sweep_config = {"name": "complete-sweep", "method": "grid"}
sweep_config["metric"] = {"name": "loss", "goal": "minimize"}

# Defining parameters for hyperparameter search
parameters_dict = {
    "num_epochs": {"values": [10, 50]},  # Number of epochs
    "size_hidden_layer": {"values": [32, 64, 128]},  # Size of hidden layer
    "optimizer": {"values": ["Normal", "Momentum", "AdaGrad", "RMSProp", "Adam", "Nadam"]},  # Optimizer
    "batch_size": {"values": [128, 1024, 60000]},  # Batch size
    "weight_init": {"values": ["RandomNormal", "XavierUniform"]},  # Weight initialization method
    "activation": {"values": ["Sigmoid", "Tanh", "Relu"]},  # Activation function
    "loss": {"values": ["CrossEntropy", "SquaredError"]}  # Loss function
}

sweep_config["parameters"] = parameters_dict

# Function to train neural network with different configurations
def train_nn(config=sweep_config):
    with wandb.init(config=config):
        config = wandb.init().config  # Getting configuration
        wandb.run.name = "e_{}_hl_{}_opt_{}_bs_{}_init_{}_ac_{}_loss_{}".format(config.num_epochs,
                                                                                  config.size_hidden_layer,
                                                                                  config.optimizer,
                                                                                  config.batch_size,
                                                                                  config.weight_init,
                                                                                  config.activation,
                                                                                  config.loss)

        # Creating neural network layers
        layers = [
            Input(data=X_scaled),  # Input layer
            Dense(size=config.size_hidden_layer, activation=config.activation, name="HL1"),  # Hidden layer
            Dense(size=10, activation=config.activation, name="OL")  # Output layer
        ]

        # Initializing neural network model
        nn_model = NeuralNetwork(
            layers=layers,  # Specifying layers
            batch_size=config.batch_size,  # Batch size for training
            optimizer=config.optimizer,  # Optimizer to be used
            initialization=config.weight_init,  # Weight initialization method
            epochs=config.num_epochs,  # Number of epochs for training
            t=t,  # Training labels
            X_val=X_val_scaled,  # Validation data
            t_val=t_val,  # Validation labels
            loss=config.loss,  # Loss function
            use_wandb=True  # Whether to use WandB for logging
        )

        # Performing forward and backward propagation
        nn_model.forward_propogation()  # Forward propagation
        nn_model.backward_propogation()  # Backward propagation

        # Evaluating performance on validation and test datasets
        acc_val, loss_val, _ = nn_model.check_test(X_val_scaled, t_val)  # Validation set evaluation
        acc_test, loss_test, _ = nn_model.check_test(X_test_scaled, t_test)  # Test set evaluation

        # Logging metrics
        wandb.log({
            "val_loss_end": loss_val / t_val.shape[1],
            "val_acc_end": acc_val / t_val.shape[1],
            "test_loss_end": loss_test / t_test.shape[1],
            "test_acc_end": acc_test / t_test.shape[1],
            "epoch": config.num_epochs
        })

# Creating hyperparameter sweep
sweep_id = wandb.sweep(sweep_config, project="trail-1")

# Running hyperparameter sweep
wandb.agent(sweep_id, function=train_nn)